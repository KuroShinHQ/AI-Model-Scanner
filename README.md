# Model EÄŸitim Raporu TarayÄ±cÄ± v2.0

## Genel BakÄ±ÅŸ

AI model eÄŸitim deneyimlerinizi, parametrelerinizi, baÅŸarÄ±/baÅŸarÄ±sÄ±zlÄ±k durumlarÄ±nÄ±zÄ± ve Ã¶ÄŸrendiÄŸiniz dersleri GitHub'da paylaÅŸmak iÃ§in tasarlanmÄ±ÅŸ kapsamlÄ± bir Python aracÄ±dÄ±r. Bu script, tÃ¼m projelerinizi otomatik olarak tarayarak:

- **EÄŸitim Parametrelerini** (optimizer, learning rate, batch size, device, quantization)
- **Performans Metriklerini** (accuracy, loss, F1 score, precision, recall)
- **BaÅŸarÄ±/BaÅŸarÄ±sÄ±zlÄ±k DurumlarÄ±nÄ±** (hangi denemeler Ã§alÄ±ÅŸtÄ±, hangilerinde sorun yaÅŸandÄ±)
- **Hata Sebeplerini** (OOM, overfitting, vb.)
- **Notlar ve GÃ¶zlemlerinizi**

analiz eder ve paylaÅŸÄ±labilir bir rapor oluÅŸturur.

## Neden Bu Tool?

AI model eÄŸitimi deneysel bir sÃ¼reÃ§tir. Hangi parametrelerin hangi koÅŸullarda baÅŸarÄ±lÄ± olduÄŸunu, hangi hatalarÄ±n neden Ã§Ä±ktÄ±ÄŸÄ±nÄ± ve neleri Ã¶ÄŸrendiÄŸimizi dokÃ¼mante etmek Ã¶nemlidir. Bu tool:

- GeÃ§miÅŸ eÄŸitim deneyimlerinizi organize eder
- BaÅŸarÄ±lÄ±/baÅŸarÄ±sÄ±z denemeleri karÅŸÄ±laÅŸtÄ±rmanÄ±zÄ± saÄŸlar
- Toplulukla bilgi paylaÅŸÄ±mÄ±nÄ± kolaylaÅŸtÄ±rÄ±r
- Hangi parametrelerin hangi modellerde Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmenizi saÄŸlar

## Ã–zellikler

### KapsamlÄ± Parametre Tespiti

- **Model Bilgileri**: Model adÄ±, mimari, base model
- **EÄŸitim AyarlarÄ±**: Epoch, sample sayÄ±sÄ±, batch size
- **Optimizer Bilgisi**: AdamW, Adam, SGD, vb.
- **Learning Rate**: 1e-5 gibi scientific notation desteÄŸi
- **Device**: cuda:0, cpu, TPU vb.
- **Quantization**: 4-bit, 8-bit, QLoRA, vb.

### Metrik Analizi

- Accuracy (train, val, test)
- Loss deÄŸerleri
- F1 Score
- Precision ve Recall
- Ä°statistiksel Ã¶zetler (ortalama, min, max)

### Durum Takibi

- âœ… **BaÅŸarÄ±lÄ± EÄŸitimler**: Tamamlanan ve baÅŸarÄ±lÄ± olan denemeler
- âŒ **BaÅŸarÄ±sÄ±z EÄŸitimler**: Hata veren veya yarÄ±da kalan denemeler
- â“ **Bilinmeyen Durum**: Status bilgisi olmayan denemeler

### Hata Analizi

- OOM (Out of Memory) hatalarÄ±
- Overfitting/underfitting durumlarÄ±
- Configuration hatalarÄ±
- Convergence problemleri
- En sÄ±k karÅŸÄ±laÅŸÄ±lan hatalar istatistiÄŸi

### Ä°statistiksel Analiz

- Model daÄŸÄ±lÄ±mÄ± (hangi modeller ne kadar kullanÄ±lmÄ±ÅŸ)
- Optimizer tercihleri
- Device kullanÄ±mÄ± (GPU/CPU)
- BaÅŸarÄ± oranÄ± analizi
- Ortalama parametre deÄŸerleri

## Kurulum

### Gereksinimler

- Python 3.7 veya Ã¼zeri
- Standart Python kÃ¼tÃ¼phaneleri (ek kurulum gerekmez!)

```python
# KullanÄ±lan kÃ¼tÃ¼phaneler - hepsi Python'la birlikte gelir
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from collections import Counter
```

### DosyayÄ± Ä°ndirme

```bash
# Git clone
git clone https://github.com/yourusername/model-training-scanner.git
cd model-training-scanner

# Veya doÄŸrudan indirin
wget https://raw.githubusercontent.com/yourusername/model-training-scanner/main/model_training_scanner.py
```

## KullanÄ±m

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

1. Script'i projenizin iÃ§inde Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python model_training_scanner.py
```

2. Raporlar otomatik olarak oluÅŸturulur:
   - `model_training_report.txt` - Okunabilir metin raporu
   - `model_training_report.json` - JSON formatÄ±nda yapÄ±landÄ±rÄ±lmÄ±ÅŸ veri

### Ã–zelleÅŸtirilmiÅŸ KullanÄ±m

Script iÃ§indeki ayarlarÄ± deÄŸiÅŸtirerek kendi projenize uyarlayÄ±n:

```python
# model_training_scanner.py iÃ§inde

# Taranacak dizin
ROOT_DIR = r"C:\Users\yourusername\your_project"

# Rapor Ã§Ä±ktÄ± yeri
OUTPUT_FILE = r"C:\Users\yourusername\your_project\report.txt"
```

### Programatik KullanÄ±m

Script'i kendi Python kodunuzda kullanabilirsiniz:

```python
from model_training_scanner import ModelTrainingScanner

# Scanner oluÅŸtur
scanner = ModelTrainingScanner(
    root_dir="/path/to/your/projects",
    output_file="training_analysis.txt"
)

# Tarama yap
scanner.run()

# Bulgulara programatik eriÅŸim
for finding in scanner.findings:
    if finding.get('status_category') == 'failed':
        print(f"BaÅŸarÄ±sÄ±z: {finding.get('model')} - {finding.get('error')}")
```

## Desteklenen Dosya FormatlarÄ±

Script aÅŸaÄŸÄ±daki dosya tÃ¼rlerinden bilgi ayÄ±klayabilir:

```
.txt, .log, .md      â†’ Metin tabanlÄ± loglar ve dokÃ¼mantasyon
.json, .yaml, .yml   â†’ YapÄ±landÄ±rma ve metrik dosyalarÄ±
.csv, .result        â†’ EÄŸitim sonuÃ§ dosyalarÄ±
.out, .metrics       â†’ Training output dosyalarÄ±
.report              â†’ Rapor dosyalarÄ±
```

## Ã–rnek Ã‡Ä±ktÄ± FormatÄ±

### Konsol Ã‡Ä±ktÄ±sÄ±

```
================================================================================
ğŸš€ MODEL EÄÄ°TÄ°M RAPORU TARAMA SONUÃ‡LARI - GELÄ°ÅMÄ°Å VERSÄ°YON
================================================================================
ğŸ“… Tarih: 2025-01-15 20:30:00
ğŸ“‚ Taranan Dizin: /home/user/ai_projects
ğŸ“„ Toplam Taranan Dosya: 1,234
âœ¨ Veri Ä°Ã§eren Dosya: 156
================================================================================

ğŸ“Š HIZLI Ä°STATÄ°STÄ°KLER:
  âœ… BaÅŸarÄ±lÄ± EÄŸitimler: 89
  âŒ BaÅŸarÄ±sÄ±z EÄŸitimler: 23
  â“ Bilinmeyen Durum: 44

================================================================================
ğŸ¯ BULGU #1: Model EÄŸitim Raporu [âŒ BAÅARISIZ]
================================================================================

ğŸ“ Dosya Bilgileri:
  â€¢ Yol: /home/user/ai_projects/gpt2_oom_attempt/training_log.txt
  â€¢ Dosya AdÄ±: training_log.txt
  â€¢ Boyut: 12.45 KB
  â€¢ DeÄŸiÅŸtirilme: 2025-01-10 14:22:33

ğŸ¤– Model Bilgileri:
  â€¢ Model: gpt2-medium
  â€¢ Epoch: 1
  â€¢ Sample SayÄ±sÄ±: 50,000

ğŸ”§ EÄŸitim Parametreleri:
  â€¢ Optimizer: AdamW
  â€¢ Learning Rate: 5.00e-05
  â€¢ Batch Size: 16
  â€¢ Device: cuda:0

ğŸ“ Durum ve Notlar:
  â€¢ Status: BaÅŸarÄ±sÄ±z
  â€¢ Hata: OOM (Out of Memory)
  â€¢ Notlar: Batch size 16 ile VRAM yetersiz. Gradient accumulation veya batch_size=8 denenmeli.

================================================================================
```

### Ä°statistiksel Analiz BÃ¶lÃ¼mÃ¼

```
================================================================================
ğŸ“ˆ DETAYLI Ä°STATÄ°STÄ°KSEL ANALÄ°Z
================================================================================

ğŸ¤– Model DaÄŸÄ±lÄ±mÄ± (Top 10):
  â€¢ gpt2: 45 eÄŸitim
  â€¢ TinyLlama-1.1B: 23 eÄŸitim
  â€¢ phi-2: 18 eÄŸitim
  â€¢ bert-base-turkish: 12 eÄŸitim

ğŸ”§ Optimizer DaÄŸÄ±lÄ±mÄ±:
  â€¢ AdamW: 89 kullanÄ±m
  â€¢ Adam: 34 kullanÄ±m
  â€¢ SGD: 12 kullanÄ±m

ğŸ’» Device DaÄŸÄ±lÄ±mÄ±:
  â€¢ cuda:0: 134 kullanÄ±m
  â€¢ cpu: 22 kullanÄ±m

âŒ En SÄ±k KarÅŸÄ±laÅŸÄ±lan Hatalar (Top 5):
  â€¢ OOM (Out of Memory)... : 15 kez
  â€¢ Loss not converging... : 8 kez
  â€¢ CUDA error: device-side assert triggered... : 5 kez

ğŸ“Š Ortalama Metrikler:
  â€¢ Accuracy: Ort=0.8567, Min=0.4523, Max=0.9823 (89 Ã¶rnek)
  â€¢ Loss: Ort=0.3421, Min=0.0234, Max=2.1234 (134 Ã¶rnek)
  â€¢ Epoch: Ort=4.2, Min=1, Max=100 (156 Ã¶rnek)
  â€¢ Batch Size: Ort=10.5, Min=1, Max=32 (112 Ã¶rnek)
  â€¢ Learning Rate: Ort=3.24e-05, Min=1.00e-06, Max=1.00e-03 (98 Ã¶rnek)

ğŸ¯ BaÅŸarÄ± OranÄ± Analizi:
  â€¢ Toplam Bilinen Durum: 112
  â€¢ BaÅŸarÄ± OranÄ±: 79.5%
  â€¢ BaÅŸarÄ±sÄ±zlÄ±k OranÄ±: 20.5%
```

## GerÃ§ek KullanÄ±m SenaryolarÄ±

### Senaryo 1: "Hangi batch size GPU'ma sÄ±ÄŸar?"

Script'iniz ÅŸunu gÃ¶sterir:
- Batch size 16 â†’ 5 OOM hatasÄ±
- Batch size 8 â†’ 3 baÅŸarÄ±lÄ± eÄŸitim
- Batch size 4 â†’ 12 baÅŸarÄ±lÄ± eÄŸitim

**SonuÃ§**: GPU'nuz iÃ§in ideal batch size = 4 veya 8

### Senaryo 2: "Hangi optimizer daha iyi?"

Ä°statistikler:
- AdamW ile ortalama accuracy: 0.89
- Adam ile ortalama accuracy: 0.84
- SGD ile ortalama accuracy: 0.79

**SonuÃ§**: AdamW bu modelde daha iyi performans gÃ¶steriyor

### Senaryo 3: "Neden model converge olmuyor?"

BaÅŸarÄ±sÄ±z eÄŸitimlerde:
- Learning rate 1e-3 â†’ 8 convergence hatasÄ±
- Learning rate 5e-5 â†’ 2 convergence hatasÄ±
- Learning rate 1e-5 â†’ 0 hata

**SonuÃ§**: Learning rate'i dÃ¼ÅŸÃ¼rmek gerekiyor

## JSON Ã‡Ä±ktÄ±sÄ±

Programatik kullanÄ±m iÃ§in JSON raporu:

```json
{
  "scan_date": "2025-01-15T20:30:00",
  "root_directory": "/home/user/ai_projects",
  "total_scanned_files": 1234,
  "files_with_data": 156,
  "statistics": {
    "total_findings": 156,
    "successful_trainings": 89,
    "failed_trainings": 23,
    "unknown_status": 44,
    "models": {
      "gpt2": 45,
      "TinyLlama-1.1B": 23
    },
    "optimizers": {
      "AdamW": 89,
      "Adam": 34
    },
    "devices": {
      "cuda:0": 134,
      "cpu": 22
    },
    "errors": {
      "OOM (Out of Memory)": 15
    },
    "avg_metrics": {
      "accuracy": {
        "mean": 0.8567,
        "min": 0.4523,
        "max": 0.9823,
        "count": 89
      }
    }
  },
  "findings": [
    {
      "file_path": "/path/to/log.txt",
      "model": "gpt2-medium",
      "epoch": 5,
      "optimizer": "AdamW",
      "learning_rate": 5e-05,
      "batch_size": 8,
      "device": "cuda:0",
      "accuracy": 0.92,
      "loss": 0.15,
      "status": "baÅŸarÄ±lÄ±",
      "status_category": "success"
    }
  ]
}
```

## Regex Pattern Ã–rnekleri

Script aÅŸaÄŸÄ±daki gibi Ã§eÅŸitli formatlarÄ± algÄ±lar:

```python
# Model adÄ±
"model: gpt2"
"model_name: bert-base-turkish"
"architecture = ResNet50"

# Parametreler
"optimizer: AdamW"
"learning_rate: 5e-5"
"lr = 0.0001"
"batch_size: 8"
"device: cuda:0"

# Quantization
"quantization: 4-bit"
"load_in_8bit: true"
"precision: fp16"

# Durum
"status: baÅŸarÄ±lÄ±"
"result: failed"
"durum: completed"

# Hata
"error: OOM"
"hata: CUDA out of memory"
"exception: RuntimeError"

# Notlar
"note: Model converge olmadÄ±, lr dÃ¼ÅŸÃ¼rÃ¼lmeli"
"notlar: Batch size 4 ile Ã§alÄ±ÅŸtÄ±"
```

## Dosya YapÄ±nÄ±z

Script tarandÄ±ÄŸÄ±nda ÅŸÃ¶yle bir yapÄ± bekler:

```
your_project/
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ gpt2_trial_1/
â”‚   â”‚   â”œâ”€â”€ training_log.txt          â† TaranÄ±r
â”‚   â”‚   â”œâ”€â”€ config.json                â† TaranÄ±r
â”‚   â”‚   â””â”€â”€ results.csv                â† TaranÄ±r
â”‚   â”‚
â”‚   â”œâ”€â”€ bert_finetuning/
â”‚   â”‚   â”œâ”€â”€ README.md                  â† TaranÄ±r
â”‚   â”‚   â””â”€â”€ metrics.log                â† TaranÄ±r
â”‚   â”‚
â”‚   â””â”€â”€ failed_attempts/
â”‚       â””â”€â”€ oom_errors.txt             â† TaranÄ±r (baÅŸarÄ±sÄ±z olarak iÅŸaretlenir)
â”‚
â”œâ”€â”€ models/                            â† Checkpoint'ler (atlanÄ±r)
â”œâ”€â”€ __pycache__/                       â† AtlanÄ±r
â””â”€â”€ .git/                              â† AtlanÄ±r
```

## GitHub'da PaylaÅŸÄ±m Ä°Ã§in Ä°puÃ§larÄ±

### 1. Raporu README'nize Ekleyin

```markdown
## Model EÄŸitim GeÃ§miÅŸi

Bu projede 156 farklÄ± eÄŸitim denemesi yapÄ±lmÄ±ÅŸtÄ±r:
- âœ… 89 baÅŸarÄ±lÄ± eÄŸitim
- âŒ 23 baÅŸarÄ±sÄ±z deneme
- ğŸ¯ %79.5 baÅŸarÄ± oranÄ±

En iyi sonuÃ§: gpt2 + AdamW + lr=5e-5 + batch_size=8 â†’ Accuracy: 0.95

DetaylÄ± rapor iÃ§in bkz: [model_training_report.txt](./model_training_report.txt)
```

### 2. Learnings BÃ¶lÃ¼mÃ¼ OluÅŸturun

```markdown
## Ã–ÄŸrendiklerim

### GPU Memory
- Batch size 16 â†’ OOM (15 deneme)
- Batch size 8 â†’ Ã‡alÄ±ÅŸÄ±yor âœ“
- Gradient accumulation kullan!

### Learning Rate
- 1e-3 â†’ Converge olmuyor
- 5e-5 â†’ En iyi sonuÃ§ âœ“
- 1e-6 â†’ Ã‡ok yavaÅŸ Ã¶ÄŸreniyor

### Quantization
- 4-bit ile %2 accuracy kaybÄ±
- Ancak 4x daha az VRAM kullanÄ±mÄ±
- Small modeller iÃ§in uygun
```

### 3. Issues OluÅŸturun

En sÄ±k hatalarÄ±nÄ±z iÃ§in GitHub Issue'larÄ± aÃ§Ä±n:

```markdown
Title: [SOLVED] OOM Error with batch_size=16
Labels: bug, solved, documentation

## Problem
gpt2-medium modeli batch_size=16 ile OOM veriyor

## Solution
- batch_size=8 kullan
- VEYA gradient_accumulation_steps=2 ekle

## Stats
15 deneme baÅŸarÄ±sÄ±z â†’ 12 deneme baÅŸarÄ±lÄ± âœ“
```

## KatkÄ±da Bulunma

### Yeni Pattern Ekleme

Kendi metriklerinizi eklemek iÃ§in:

```python
# model_training_scanner.py iÃ§inde PATTERNS sÃ¶zlÃ¼ÄŸÃ¼ne ekleyin

'your_metric': [
    r'your_metric[:\s=]+([0-9]*\.?[0-9]+)',
    r'alternative_name[:\s=]+([0-9]*\.?[0-9]+)',
],
```

### Yeni Dosya FormatÄ± DesteÄŸi

```python
# SUPPORTED_EXTENSIONS listesine ekleyin
SUPPORTED_EXTENSIONS = [
    '.txt', '.log', '.md', '.json', '.yaml',
    '.your_new_format'  # Yeni format
]
```

## Lisans

Bu proje aÃ§Ä±k kaynaklÄ±dÄ±r (MIT License). Ä°stediÄŸiniz gibi kullanabilir, deÄŸiÅŸtirebilir ve paylaÅŸabilirsiniz.

## Yazar & Ä°letiÅŸim

**Kuroshin AI Project**

- GitHub: [@yourusername](https://github.com/yourusername)
- Proje: KuroshinPro AI Platform

## Changelog

### v2.0 (2025-01-15) - GitHub PaylaÅŸÄ±m Versiyonu

**Yeni Ã–zellikler:**
- âœ¨ EÄŸitim parametreleri tespiti (optimizer, lr, batch_size, device, quantization)
- âœ¨ BaÅŸarÄ±/baÅŸarÄ±sÄ±zlÄ±k durumu analizi
- âœ¨ Hata sebepleri ve notlar ayÄ±klama
- âœ¨ DetaylÄ± istatistiksel analiz
- âœ¨ BaÅŸarÄ± oranÄ± hesaplama
- âœ¨ En sÄ±k hatalar listesi
- âœ¨ Model/optimizer/device daÄŸÄ±lÄ±mÄ±

**GeliÅŸtirmeler:**
- ğŸ”§ GeliÅŸmiÅŸ regex pattern'leri
- ğŸ”§ JSON ve text dosyalarÄ± iÃ§in Ã¶zel parsing
- ğŸ”§ TÃ¼rkÃ§e keyword desteÄŸi
- ğŸ”§ Scientific notation support (1e-5)
- ğŸ”§ Status kategorileme (success/failed/unknown)

**DÃ¼zeltmeler:**
- ğŸ› JSON array hatalarÄ±
- ğŸ› Encoding sorunlarÄ±
- ğŸ› Tuple deÄŸer ayÄ±klama

### v1.0 (2025-01-10) - Ä°lk SÃ¼rÃ¼m
- Temel tarama Ã¶zellikleri
- Model adÄ±, epoch, sample tespiti
- Accuracy, loss, F1 metrikleri
- JSON ve text rapor Ã§Ä±ktÄ±sÄ±

---

**ğŸ’¡ Pro Tip**: Bu tool'u dÃ¼zenli aralÄ±klarla Ã§alÄ±ÅŸtÄ±rarak eÄŸitim geÃ§miÅŸinizi takip edin. Her deneme bir Ã¶ÄŸrenme fÄ±rsatÄ±dÄ±r!

**ğŸ¯ Hedef**: AI modellerinizi eÄŸitirken Ã¶ÄŸrendiklerinizi dokÃ¼mante edin ve toplulukla paylaÅŸÄ±n!
