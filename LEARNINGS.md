# Model EÄŸitim Deneyimleri ve Ã–ÄŸrenilenler

Bu dokÃ¼man, KuroshinPro AI Platform geliÅŸtirme sÃ¼recinde yapÄ±lan model eÄŸitim denemelerinden edinilen tecrÃ¼beleri, baÅŸarÄ±lÄ± stratejileri ve karÅŸÄ±laÅŸÄ±lan sorunlarÄ± iÃ§erir.

## Genel Ä°statistikler

ğŸ“Š **Toplam Analiz:**
- **Taranan Dosya**: 9,109
- **EÄŸitim Verisi Ä°Ã§eren Dosya**: 3,036
- **Benzersiz Model Denemesi**: 3,965

ğŸ¯ **BaÅŸarÄ± Durumu:**
- âœ… BaÅŸarÄ±lÄ± EÄŸitimler: 87 (10.3%)
- âŒ BaÅŸarÄ±sÄ±z/Sorunlu: 754 (89.7%)
- â“ Durum Belirsiz: 3,124

> **Not**: YÃ¼ksek baÅŸarÄ±sÄ±zlÄ±k oranÄ±, deneysel geliÅŸtirme sÃ¼recinin doÄŸal bir parÃ§asÄ±dÄ±r. Her baÅŸarÄ±sÄ±zlÄ±k yeni bir Ã¶ÄŸrenme fÄ±rsatÄ±dÄ±r!

## En Ã‡ok KullanÄ±lan Modeller

### Top 5 Model

1. **GPT-2** â†’ 219 deneme
   - En stabil ve test edilmiÅŸ model
   - TÃ¼rkÃ§e fine-tuning iÃ§in uygun
   - Resource-efficient

2. **EleutherAI/gpt-neo-125M** â†’ 172 deneme
   - KÃ¼Ã§Ã¼k ama gÃ¼Ã§lÃ¼ alternatif
   - GPT-2'den daha iyi performans
   - CPU'da bile Ã§alÄ±ÅŸabilir

3. **Kuroshin Small 1.3B** â†’ 72 deneme
   - Ã–zel geliÅŸtirilmiÅŸ model
   - TÃ¼rkÃ§e optimize
   - Ä°yi accuracy/size oranÄ±

4. **TinyLlama 1.1B** â†’ 40 deneme
   - Son zamanlarda popÃ¼ler
   - HÄ±zlÄ± eÄŸitim
   - DÃ¼ÅŸÃ¼k memory footprint

5. **Microsoft Phi-2** â†’ 18 deneme
   - Ã‡ok yeni denemeler
   - Promising sonuÃ§lar
   - Daha fazla test gerekiyor

## Optimizer Deneyimleri

### En BaÅŸarÄ±lÄ± Optimizers

**AdamW** âœ…
- Neredeyse tÃ¼m baÅŸarÄ±lÄ± eÄŸitimlerde kullanÄ±ldÄ±
- Learning rate'e toleranslÄ±
- Default choice olarak Ã¶neriliyor

**Adam** âš ï¸
- AdamW'den biraz daha az stabil
- BazÄ± modellerde overfitting
- Weight decay ile birlikte kullanÄ±lmalÄ±

**SGD** âŒ
- Genelde yavaÅŸ convergence
- Daha aggressive learning rate scheduler gerekiyor
- Momentum ile kullanÄ±lmazsa zor

## Device & Hardware

### GPU KullanÄ±mÄ±

**CUDA (GPU)** â†’ 429 kullanÄ±m
- AÃ§Ä±k ara en Ã§ok kullanÄ±lan
- 10-50x hÄ±zlanma
- OOM hatalarÄ± en bÃ¼yÃ¼k sorun

**CPU** â†’ 28 kullanÄ±m
- KÃ¼Ã§Ã¼k modeller iÃ§in OK
- Test ve debug iÃ§in kullanÄ±ÅŸlÄ±
- Production iÃ§in Ã§ok yavaÅŸ

### OOM (Out of Memory) HatalarÄ±

En sÄ±k karÅŸÄ±laÅŸÄ±lan sorun! **45+ OOM hatasÄ±**

#### Ã‡Ã¶zÃ¼mler:

```python
# âŒ BAÅARISIZ
batch_size = 16  # OOM!
gradient_accumulation_steps = 1

# âœ… BAÅARILI
batch_size = 4  # veya 8
gradient_accumulation_steps = 4  # Effective batch = 16
```

#### Memory Optimizasyon Taktikleri:

1. **Quantization Kullan**
   ```python
   load_in_8bit=True  # %50 memory tasarrufu
   load_in_4bit=True  # %75 memory tasarrufu
   ```

2. **Gradient Checkpointing**
   ```python
   model.gradient_checkpointing_enable()
   # Memory â†“30%, Speed â†“20%
   ```

3. **Mixed Precision Training**
   ```python
   from torch.cuda.amp import autocast
   # Memory â†“40%, Speed â†‘30%
   ```

## Learning Rate Stratejileri

### Ã–ÄŸrenilenler

ğŸ“‰ **Ã‡ok YÃ¼ksek LR (1e-3)**
- Loss explode ediyor
- Model converge olmuyor
- NaN deÄŸerleri oluÅŸuyor

âœ… **Optimal Range (5e-5 to 1e-4)**
- Stabil eÄŸitim
- Ä°yi convergence
- Ã‡oÄŸu model iÃ§in ideal

ğŸ“ˆ **Ã‡ok DÃ¼ÅŸÃ¼k LR (1e-6)**
- Ã‡ok yavaÅŸ Ã¶ÄŸrenme
- Sonsuz epoch gerekiyor
- SabÄ±r testi!

### Learning Rate Scheduler

**Warmup + Cosine Annealing** â†’ En baÅŸarÄ±lÄ± stratejÄ°

```python
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,  # Total steps'in %10'u
    num_training_steps=1000
)
```

## Batch Size Deneyimleri

### GPU Memory'e GÃ¶re Ã–neriler

| GPU VRAM | Model Size | Ã–nerilen Batch Size |
|----------|-----------|---------------------|
| 4GB      | Small (<500M) | 1-2 |
| 8GB      | Small-Medium | 4-8 |
| 12GB     | Medium (1B) | 8-16 |
| 16GB+    | Large (3B+) | 16-32 |

### Gradient Accumulation

KÃ¼Ã§Ã¼k batch size kullanÄ±yorsanÄ±z mutlaka gradient accumulation ekleyin:

```python
effective_batch_size = batch_size * gradient_accumulation_steps

# Ã–rnek:
batch_size = 4
gradient_accumulation_steps = 8
# â†’ Effective batch = 32
```

## Quantization Deneyimleri

### 4-bit Quantization (QLoRA)

**Avantajlar:**
- âœ… %75 memory tasarrufu
- âœ… BÃ¼yÃ¼k modelleri kÃ¼Ã§Ã¼k GPU'larda Ã§alÄ±ÅŸtÄ±rma
- âœ… Hala fine-tune edilebilir

**Dezavantajlar:**
- âŒ %2-5 accuracy kaybÄ±
- âŒ Biraz daha yavaÅŸ
- âŒ Inference iÃ§in dequantization gerekebilir

**Ã–rnek:**
```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=bnb_config
)
```

### 8-bit Quantization

- Daha dengeli: %50 memory, %1-2 accuracy kaybÄ±
- Production iÃ§in daha uygun
- HÄ±zlÄ± inference

## En SÄ±k Hatalar ve Ã‡Ã¶zÃ¼mleri

### 1. CUDA Out of Memory (OOM)

**Belirtiler:**
```
RuntimeError: CUDA out of memory.
Tried to allocate 2.50 GiB
```

**Ã‡Ã¶zÃ¼mler:**
1. Batch size'Ä± dÃ¼ÅŸÃ¼r
2. Gradient accumulation kullan
3. Gradient checkpointing aÃ§
4. Quantization kullan
5. Model'i deÄŸiÅŸtir (daha kÃ¼Ã§Ã¼k)

### 2. Loss Not Converging

**Belirtiler:**
- Loss dÃ¼ÅŸmÃ¼yor
- Veya Ã§ok yavaÅŸ dÃ¼ÅŸÃ¼yor
- Plateau yapÄ±yor

**Ã‡Ã¶zÃ¼mler:**
1. Learning rate'i ayarla
2. Warmup steps ekle
3. Data quality'yi kontrol et
4. Overfitting var mÄ± kontrol et

### 3. NaN Loss

**Belirtiler:**
```
Step 234: loss = nan
```

**Ã‡Ã¶zÃ¼mler:**
1. Learning rate'i dÃ¼ÅŸÃ¼r (genelde bu!)
2. Gradient clipping kullan
3. Mixed precision'Ä± kapat
4. Data'da NaN/Inf var mÄ± kontrol et

### 4. Model Overfitting

**Belirtiler:**
- Train accuracy yÃ¼ksek
- Val accuracy dÃ¼ÅŸÃ¼k
- Loss gap artÄ±yor

**Ã‡Ã¶zÃ¼mler:**
1. Dropout ekle/arttÄ±r
2. Weight decay kullan
3. Data augmentation
4. Daha fazla data
5. Regularization teknikleri

## BaÅŸarÄ±lÄ± KonfigÃ¼rasyonlar

### Configuration #1: Small Model Fast Training

```python
# Model: GPT-2 (124M)
# Use case: Prototyping, testing

config = {
    "model": "gpt2",
    "batch_size": 8,
    "gradient_accumulation_steps": 4,
    "learning_rate": 5e-5,
    "epochs": 3,
    "optimizer": "AdamW",
    "scheduler": "cosine_with_warmup",
    "warmup_ratio": 0.1,
}

# SonuÃ§: âœ… 2 saat, good accuracy
```

### Configuration #2: Medium Model Production

```python
# Model: GPT-Neo 1.3B
# Use case: Production deployment

config = {
    "model": "EleutherAI/gpt-neo-1.3B",
    "load_in_8bit": True,
    "batch_size": 4,
    "gradient_accumulation_steps": 8,
    "learning_rate": 3e-5,
    "epochs": 5,
    "optimizer": "AdamW",
    "weight_decay": 0.01,
    "scheduler": "cosine_with_warmup",
    "warmup_steps": 500,
    "gradient_checkpointing": True,
}

# SonuÃ§: âœ… 12 saat, excellent accuracy
```

### Configuration #3: LoRA Fine-tuning

```python
# Model: Any large model
# Use case: Parameter-efficient fine-tuning

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # Rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

config = {
    "model": "large-model",
    "lora": lora_config,
    "batch_size": 8,
    "learning_rate": 1e-4,  # LoRA iÃ§in biraz daha yÃ¼ksek OK
    "epochs": 5,
}

# SonuÃ§: âœ… Sadece %1 parameter train, %95 orijinal accuracy
```

## Dataset Ä°puÃ§larÄ±

### Optimal Dataset Size

| Model Size | Min Samples | Optimal Samples |
|-----------|-------------|-----------------|
| Small (125M) | 1,000 | 10,000+ |
| Medium (1B) | 10,000 | 100,000+ |
| Large (3B+) | 50,000 | 500,000+ |

### Data Quality > Quantity

**Ã–ÄŸrenilen:**
- 10K high-quality > 100K noisy data
- Data cleaning Ã§ok Ã¶nemli
- Balanced dataset ÅŸart
- Validation split unutma! (10-20%)

## Gelecek Denemeler

### PlanlanÄ±yor:

1. **Mixtral 8x7B** with extreme quantization
2. **GPT-4 distillation** kÃ¼Ã§Ã¼k modellere
3. **Multi-task learning** approach
4. **Curriculum learning** strategies
5. **Better TÃ¼rkÃ§e tokenization**

### Yeni Teknikler:

- [ ] QLoRA + Flash Attention
- [ ] Parameter-Efficient Tuning methods
- [ ] Retrieval-Augmented Generation (RAG)
- [ ] Constitutional AI principles
- [ ] Multi-modal models (text + image)

## Kaynaklar ve Referanslar

### YararlÄ± Linkler:

- [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers)
- [PEFT Library](https://github.com/huggingface/peft)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [EleutherAI](https://www.eleuther.ai/)

### KullanÄ±lan Tools:

- PyTorch / TensorFlow
- HuggingFace Transformers
- Weights & Biases (tracking)
- TensorBoard
- DeepSpeed (distributed training)

## KatkÄ±da Bulunanlar

Bu learnings dokÃ¼manÄ±, 3,965 farklÄ± eÄŸitim denemesinin analiziyle oluÅŸturulmuÅŸtur.

**Proje:** KuroshinPro AI Platform
**Tool:** Model Training Scanner v2.0
**Son GÃ¼ncelleme:** 2025-01-15

---

ğŸ’¡ **Pro Tip**: Bu dokÃ¼manÄ± dÃ¼zenli gÃ¼ncelleyin! Her yeni deneme yeni bir Ã¶ÄŸrenme.

ğŸ¯ **Hedef**: 100+ baÅŸarÄ±lÄ± eÄŸitim, < %20 baÅŸarÄ±sÄ±zlÄ±k oranÄ±

ğŸ“Š **Metric**: Her ay geliÅŸimi takip et!
